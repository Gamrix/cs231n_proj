04:38:41    Loading dataset...
04:38:41    	On dir 0 of 161
04:38:41    	On dir 1 of 161
04:38:41    	On dir 2 of 161
04:38:46    	On dir 3 of 161
04:38:46    	On dir 4 of 161
04:38:46    	On dir 5 of 161
04:38:47    	On dir 6 of 161
04:38:47    	On dir 7 of 161
04:38:48    	On dir 8 of 161
04:38:48    	On dir 9 of 161
04:38:48    	On dir 10 of 161
04:38:48    	On dir 11 of 161
04:38:50    	On dir 12 of 161
04:38:53    	On dir 13 of 161
04:38:57    	On dir 14 of 161
04:39:00    	On dir 15 of 161
04:39:00    	On dir 16 of 161
04:39:00    	On dir 17 of 161
04:39:01    	On dir 18 of 161
04:39:04    	On dir 19 of 161
04:39:05    	On dir 20 of 161
04:39:06    	On dir 21 of 161
04:39:07    	On dir 22 of 161
04:39:09    	On dir 23 of 161
04:39:09    	On dir 24 of 161
04:39:09    	On dir 25 of 161
04:39:10    	On dir 26 of 161
04:39:10    	On dir 27 of 161
04:39:10    	On dir 28 of 161
04:39:10    	On dir 29 of 161
04:39:10    	On dir 30 of 161
04:39:10    	On dir 31 of 161
04:39:10    	On dir 32 of 161
04:39:10    	On dir 33 of 161
04:39:11    	On dir 34 of 161
04:39:11    	On dir 35 of 161
04:39:11    	On dir 36 of 161
04:39:15    	On dir 37 of 161
04:39:15    	On dir 38 of 161
04:39:17    	On dir 39 of 161
04:39:18    	On dir 40 of 161
04:39:18    	On dir 41 of 161
04:39:18    	On dir 42 of 161
04:39:18    	On dir 43 of 161
04:39:18    	On dir 44 of 161
04:39:18    	On dir 45 of 161
04:39:19    	On dir 46 of 161
04:39:19    	On dir 47 of 161
04:39:19    	On dir 48 of 161
04:39:20    	On dir 49 of 161
04:39:20    	On dir 50 of 161
04:39:20    	On dir 51 of 161
04:39:20    	On dir 52 of 161
04:39:20    	On dir 53 of 161
04:39:20    	On dir 54 of 161
04:39:23    	On dir 55 of 161
04:39:23    	On dir 56 of 161
04:39:23    	On dir 57 of 161
04:39:23    	On dir 58 of 161
04:39:47    	On dir 59 of 161
04:39:47    	On dir 60 of 161
04:39:47    	On dir 61 of 161
04:39:48    	On dir 62 of 161
04:39:48    	On dir 63 of 161
04:39:49    	On dir 64 of 161
04:39:50    	On dir 65 of 161
04:39:52    	On dir 66 of 161
04:39:52    	On dir 67 of 161
04:39:52    	On dir 68 of 161
04:39:52    	On dir 69 of 161
04:39:54    	On dir 70 of 161
04:39:58    	On dir 71 of 161
04:40:02    	On dir 72 of 161
04:40:03    	On dir 73 of 161
04:40:03    	On dir 74 of 161
04:40:04    	On dir 75 of 161
04:40:05    	On dir 76 of 161
04:40:07    	On dir 77 of 161
04:40:10    	On dir 78 of 161
04:40:12    	On dir 79 of 161
04:40:12    	On dir 80 of 161
04:40:13    	On dir 81 of 161
04:40:13    	On dir 82 of 161
04:40:14    	On dir 83 of 161
04:40:14    	On dir 84 of 161
04:40:34    	On dir 85 of 161
04:40:34    	On dir 86 of 161
04:40:38    	On dir 87 of 161
04:40:45    	On dir 88 of 161
04:40:45    	On dir 89 of 161
04:40:45    	On dir 90 of 161
04:40:50    	On dir 91 of 161
04:40:52    	On dir 92 of 161
04:40:53    	On dir 93 of 161
04:40:54    	On dir 94 of 161
04:40:54    	On dir 95 of 161
04:40:54    	On dir 96 of 161
04:41:06    	On dir 97 of 161
04:41:08    	On dir 98 of 161
04:41:09    	On dir 99 of 161
04:41:10    	On dir 100 of 161
04:41:10    	On dir 101 of 161
04:41:10    	On dir 102 of 161
04:41:11    	On dir 103 of 161
04:41:11    	On dir 104 of 161
04:41:11    	On dir 105 of 161
04:41:13    	On dir 106 of 161
04:41:13    	On dir 107 of 161
04:41:13    	On dir 108 of 161
04:41:15    	On dir 109 of 161
04:41:15    	On dir 110 of 161
04:41:15    	On dir 111 of 161
04:41:18    	On dir 112 of 161
04:41:19    	On dir 113 of 161
04:41:20    	On dir 114 of 161
04:41:20    	On dir 115 of 161
04:41:26    	On dir 116 of 161
04:41:26    	On dir 117 of 161
04:41:26    	On dir 118 of 161
04:41:26    	On dir 119 of 161
04:41:26    	On dir 120 of 161
04:41:27    	On dir 121 of 161
04:41:29    	On dir 122 of 161
04:41:30    	On dir 123 of 161
04:41:32    	On dir 124 of 161
04:41:33    	On dir 125 of 161
04:41:33    	On dir 126 of 161
04:41:33    	On dir 127 of 161
04:41:34    	On dir 128 of 161
04:41:35    	On dir 129 of 161
04:41:35    	On dir 130 of 161
04:41:36    	On dir 131 of 161
04:41:38    	On dir 132 of 161
04:41:38    	On dir 133 of 161
04:41:39    	On dir 134 of 161
04:41:40    	On dir 135 of 161
04:41:40    	On dir 136 of 161
04:41:40    	On dir 137 of 161
04:41:41    	On dir 138 of 161
04:41:41    	On dir 139 of 161
04:41:41    	On dir 140 of 161
04:41:42    	On dir 141 of 161
04:41:42    	On dir 142 of 161
04:41:43    	On dir 143 of 161
04:41:43    	On dir 144 of 161
04:41:43    	On dir 145 of 161
04:41:43    	On dir 146 of 161
04:41:43    	On dir 147 of 161
04:41:44    	On dir 148 of 161
04:41:44    	On dir 149 of 161
04:41:46    	On dir 150 of 161
04:41:46    	On dir 151 of 161
04:42:06    	On dir 152 of 161
04:42:06    	On dir 153 of 161
04:42:06    	On dir 154 of 161
04:42:07    	On dir 155 of 161
04:42:07    	On dir 156 of 161
04:42:08    	On dir 157 of 161
04:42:08    	On dir 158 of 161
04:42:08    	On dir 159 of 161
04:42:09    	On dir 160 of 161
04:42:19    not saving vars due to disk constraints
04:42:20    Making loaders...
04:42:20    Beginning to run model...
04:42:23    Starting epoch 1 / 2...
04:42:24    Running evaluation...
04:42:39    Eval norm l2 loss: 1.0851, norm total loss: 1.0851
04:43:03    	training: t = 20, loss = 8142.1240, norm_loss= 1.0851
04:43:27    	training: t = 40, loss = 8784.3965, norm_loss= 1.1090
04:43:52    	training: t = 60, loss = 13915.3340, norm_loss= 0.9754
04:44:16    	training: t = 80, loss = 10284.4883, norm_loss= 0.8085
04:44:41    	training: t = 100, loss = 8865.8535, norm_loss= 0.8508
04:45:06    	training: t = 120, loss = 9691.8516, norm_loss= 0.7398
04:45:30    	training: t = 140, loss = 6528.2700, norm_loss= 0.7505
04:45:55    	training: t = 160, loss = 6169.1416, norm_loss= 0.7141
04:46:20    	training: t = 180, loss = 6736.4312, norm_loss= 0.6324
04:46:30    Running evaluation...
04:46:45    Eval norm l2 loss: 0.7208, norm total loss: 0.7208
04:47:00    	training: t = 200, loss = 5727.8325, norm_loss= 0.7460
04:47:24    	training: t = 220, loss = 3079.7410, norm_loss= 0.6073
04:47:49    	training: t = 240, loss = 8740.0742, norm_loss= 0.7379
04:48:14    	training: t = 260, loss = 10643.6484, norm_loss= 0.6500
04:48:38    	training: t = 280, loss = 9448.6553, norm_loss= 0.7493
04:49:03    	training: t = 300, loss = 9509.7549, norm_loss= 0.8118
04:49:28    	training: t = 320, loss = 8619.1348, norm_loss= 0.7156
04:49:52    	training: t = 340, loss = 6311.9282, norm_loss= 0.7419
04:50:17    	training: t = 360, loss = 9372.5742, norm_loss= 0.6984
04:50:36    Running evaluation...
04:50:51    Eval norm l2 loss: 0.7235, norm total loss: 0.7235
04:50:57    	training: t = 380, loss = 5418.9390, norm_loss= 0.6894
04:51:22    	training: t = 400, loss = 10359.0625, norm_loss= 0.6258
04:51:46    	training: t = 420, loss = 3660.7483, norm_loss= 0.6972
04:52:11    	training: t = 440, loss = 5431.5527, norm_loss= 0.6183
04:52:36    	training: t = 460, loss = 9108.2930, norm_loss= 0.6427
04:53:00    	training: t = 480, loss = 7182.1924, norm_loss= 0.6327
04:53:25    	training: t = 500, loss = 5199.8872, norm_loss= 0.5928
04:53:50    	training: t = 520, loss = 5849.1479, norm_loss= 0.7272
04:54:15    	training: t = 540, loss = 7074.7544, norm_loss= 0.6353
04:54:39    	training: t = 560, loss = 10148.5586, norm_loss= 0.6925
04:54:42    Running evaluation...
04:54:57    Eval norm l2 loss: 0.6475, norm total loss: 0.6475
04:55:19    	training: t = 580, loss = 8678.6807, norm_loss= 0.7919
04:55:43    	training: t = 600, loss = 5481.9912, norm_loss= 0.7077
04:56:08    	training: t = 620, loss = 7651.2124, norm_loss= 0.6657
04:56:33    	training: t = 640, loss = 6205.2856, norm_loss= 0.6075
04:56:58    	training: t = 660, loss = 3396.3918, norm_loss= 0.6290
04:57:22    	training: t = 680, loss = 5211.3374, norm_loss= 0.5081
04:57:47    	training: t = 700, loss = 8485.9004, norm_loss= 0.6619
04:58:12    	training: t = 720, loss = 7633.0371, norm_loss= 0.6780
04:58:36    	training: t = 740, loss = 5157.8374, norm_loss= 0.5877
04:58:47    Running evaluation...
04:59:03    Eval norm l2 loss: 0.5999, norm total loss: 0.5999
04:59:16    	training: t = 760, loss = 5986.5249, norm_loss= 0.6750
04:59:41    	training: t = 780, loss = 1813.8796, norm_loss= 0.4396
05:00:05    	training: t = 800, loss = 4569.2720, norm_loss= 0.5092
05:00:30    	training: t = 820, loss = 6753.9268, norm_loss= 0.5057
05:00:55    	training: t = 840, loss = 4400.7036, norm_loss= 0.5072
05:01:19    	training: t = 860, loss = 6460.4775, norm_loss= 0.5582
05:01:44    	training: t = 880, loss = 4898.6860, norm_loss= 0.4503
05:02:09    	training: t = 900, loss = 10268.2568, norm_loss= 0.7796
05:02:34    	training: t = 920, loss = 3697.9302, norm_loss= 0.6591
05:02:53    Running evaluation...
05:03:08    Eval norm l2 loss: 0.5336, norm total loss: 0.5336
05:03:13    	training: t = 940, loss = 2598.3945, norm_loss= 0.5325
05:03:39    	training: t = 960, loss = 5593.9722, norm_loss= 0.5171
05:04:03    	training: t = 980, loss = 4463.8804, norm_loss= 0.4521
05:04:27    	training: t = 1000, loss = 5084.5879, norm_loss= 0.4968
05:04:52    	training: t = 1020, loss = 4669.9917, norm_loss= 0.4360
05:05:17    	training: t = 1040, loss = 5228.4150, norm_loss= 0.5560
05:05:41    	training: t = 1060, loss = 6284.5024, norm_loss= 0.6167
05:06:06    	training: t = 1080, loss = 4980.3877, norm_loss= 0.6248
05:06:31    	training: t = 1100, loss = 5137.2007, norm_loss= 0.5651
05:06:56    	training: t = 1120, loss = 3558.9141, norm_loss= 0.5848
05:06:59    Running evaluation...
05:07:14    Eval norm l2 loss: 0.5053, norm total loss: 0.5053
05:07:35    	training: t = 1140, loss = 4965.5918, norm_loss= 0.5014
05:08:00    	training: t = 1160, loss = 2685.2756, norm_loss= 0.4190
05:08:25    	training: t = 1180, loss = 6439.5034, norm_loss= 0.5283
05:08:49    	training: t = 1200, loss = 5569.9277, norm_loss= 0.4599
05:09:14    	training: t = 1220, loss = 4918.2676, norm_loss= 0.5112
05:09:39    	training: t = 1240, loss = 3248.6396, norm_loss= 0.4692
05:10:04    	training: t = 1260, loss = 5244.9658, norm_loss= 0.5075
05:10:28    	training: t = 1280, loss = 6609.4009, norm_loss= 0.4831
05:10:53    	training: t = 1300, loss = 7209.4907, norm_loss= 0.4638
05:11:05    Running evaluation...
05:11:20    Eval norm l2 loss: 0.4850, norm total loss: 0.4850
05:11:33    	training: t = 1320, loss = 6046.1045, norm_loss= 0.4367
05:11:57    	training: t = 1340, loss = 4242.2671, norm_loss= 0.4826
05:12:22    	training: t = 1360, loss = 2596.9629, norm_loss= 0.5575
05:12:47    	training: t = 1380, loss = 4335.6460, norm_loss= 0.3921
05:13:12    	training: t = 1400, loss = 4288.7720, norm_loss= 0.4249
05:13:36    	training: t = 1420, loss = 4982.7324, norm_loss= 0.5250
05:14:01    	training: t = 1440, loss = 4482.0820, norm_loss= 0.4484
05:14:26    	training: t = 1460, loss = 5951.8008, norm_loss= 0.5361
05:14:50    	training: t = 1480, loss = 4273.8423, norm_loss= 0.4054
05:15:11    Running evaluation...
05:15:26    Eval norm l2 loss: 0.4574, norm total loss: 0.4574
05:15:30    	training: t = 1500, loss = 4363.4805, norm_loss= 0.4797
05:15:31    Starting epoch 2 / 2...
05:15:31    Running evaluation...
05:15:47    Eval norm l2 loss: 0.4611, norm total loss: 0.4611
05:16:10    	training: t = 20, loss = 6701.0068, norm_loss= 0.4558
05:16:35    	training: t = 40, loss = 5220.8545, norm_loss= 0.5729
05:16:59    	training: t = 60, loss = 2169.7483, norm_loss= 0.3108
05:17:24    	training: t = 80, loss = 5298.1416, norm_loss= 0.3755
05:17:49    	training: t = 100, loss = 4489.5938, norm_loss= 0.4383
05:18:13    	training: t = 120, loss = 6758.9717, norm_loss= 0.4297
05:18:38    	training: t = 140, loss = 4213.1997, norm_loss= 0.4969
05:19:03    	training: t = 160, loss = 4446.4150, norm_loss= 0.4623
05:19:27    	training: t = 180, loss = 4189.0615, norm_loss= 0.4568
05:19:37    Running evaluation...
05:19:52    Eval norm l2 loss: 0.4899, norm total loss: 0.4899
05:20:07    	training: t = 200, loss = 5005.5928, norm_loss= 0.3565
05:20:32    	training: t = 220, loss = 5108.7119, norm_loss= 0.3844
05:20:57    	training: t = 240, loss = 3867.1521, norm_loss= 0.3347
05:21:21    	training: t = 260, loss = 5835.9922, norm_loss= 0.4039
05:21:46    	training: t = 280, loss = 4783.8828, norm_loss= 0.4425
05:22:11    	training: t = 300, loss = 3800.7349, norm_loss= 0.3651
05:22:35    	training: t = 320, loss = 3312.7798, norm_loss= 0.5343
05:23:00    	training: t = 340, loss = 6945.2979, norm_loss= 0.5597
05:23:25    	training: t = 360, loss = 4036.5173, norm_loss= 0.4131
05:23:43    Running evaluation...
05:23:58    Eval norm l2 loss: 0.4691, norm total loss: 0.4691
05:24:05    	training: t = 380, loss = 3516.7695, norm_loss= 0.4230
05:24:29    	training: t = 400, loss = 4265.3291, norm_loss= 0.4247
05:24:54    	training: t = 420, loss = 2578.4968, norm_loss= 0.4365
05:25:19    	training: t = 440, loss = 4485.7305, norm_loss= 0.4267
05:25:43    	training: t = 460, loss = 4067.6431, norm_loss= 0.4008
05:26:08    	training: t = 480, loss = 6043.1582, norm_loss= 0.4991
05:26:33    	training: t = 500, loss = 2022.0306, norm_loss= 0.5449
05:26:57    	training: t = 520, loss = 4141.5435, norm_loss= 0.4640
05:27:22    	training: t = 540, loss = 3316.5251, norm_loss= 0.3307
05:27:47    	training: t = 560, loss = 3302.6841, norm_loss= 0.4367
05:27:49    Running evaluation...
05:28:04    Eval norm l2 loss: 0.4610, norm total loss: 0.4610
05:28:27    	training: t = 580, loss = 3286.5393, norm_loss= 0.3885
05:28:51    	training: t = 600, loss = 2365.5759, norm_loss= 0.4020
05:29:16    	training: t = 620, loss = 4072.6162, norm_loss= 0.4253
05:29:41    	training: t = 640, loss = 4199.6992, norm_loss= 0.3845
05:30:05    	training: t = 660, loss = 5688.4844, norm_loss= 0.4398
05:30:30    	training: t = 680, loss = 1783.3474, norm_loss= 0.3800
05:30:55    	training: t = 700, loss = 5486.2773, norm_loss= 0.3919
05:31:19    	training: t = 720, loss = 5462.8467, norm_loss= 0.4383
05:31:44    	training: t = 740, loss = 2975.7939, norm_loss= 0.3414
05:31:55    Running evaluation...
05:32:10    Eval norm l2 loss: 0.4093, norm total loss: 0.4093
05:32:24    	training: t = 760, loss = 6480.0840, norm_loss= 0.4787
05:32:48    	training: t = 780, loss = 2282.0979, norm_loss= 0.3108
05:33:13    	training: t = 800, loss = 4879.5122, norm_loss= 0.4470
05:33:38    	training: t = 820, loss = 3739.3562, norm_loss= 0.2939
05:34:02    	training: t = 840, loss = 3412.9075, norm_loss= 0.4017
05:34:27    	training: t = 860, loss = 3904.5994, norm_loss= 0.3863
05:34:52    	training: t = 880, loss = 4271.4297, norm_loss= 0.4703
05:35:16    	training: t = 900, loss = 3820.1416, norm_loss= 0.3866
05:35:41    	training: t = 920, loss = 3273.0615, norm_loss= 0.2890
05:36:01    Running evaluation...
05:36:16    Eval norm l2 loss: 0.4068, norm total loss: 0.4068
05:36:21    	training: t = 940, loss = 5411.7490, norm_loss= 0.5308
05:36:46    	training: t = 960, loss = 5126.8247, norm_loss= 0.4380
05:37:11    	training: t = 980, loss = 4977.9731, norm_loss= 0.3532
05:37:35    	training: t = 1000, loss = 4379.1895, norm_loss= 0.5466
05:38:00    	training: t = 1020, loss = 3956.2009, norm_loss= 0.4358
05:38:24    	training: t = 1040, loss = 6455.7056, norm_loss= 0.4132
05:38:49    	training: t = 1060, loss = 3851.6946, norm_loss= 0.4168
05:39:14    	training: t = 1080, loss = 4728.5508, norm_loss= 0.4076
05:39:38    	training: t = 1100, loss = 2185.3142, norm_loss= 0.3326
05:40:03    	training: t = 1120, loss = 4827.1270, norm_loss= 0.3809
05:40:07    Running evaluation...
05:40:22    Eval norm l2 loss: 0.4056, norm total loss: 0.4056
05:40:43    	training: t = 1140, loss = 2403.3635, norm_loss= 0.4177
05:41:08    	training: t = 1160, loss = 2644.7781, norm_loss= 0.3161
05:41:32    	training: t = 1180, loss = 3874.8057, norm_loss= 0.3519
05:41:57    	training: t = 1200, loss = 4187.1274, norm_loss= 0.3745
05:42:22    	training: t = 1220, loss = 2076.7576, norm_loss= 0.3088
05:42:46    	training: t = 1240, loss = 3377.5952, norm_loss= 0.3224
05:43:11    	training: t = 1260, loss = 6064.5264, norm_loss= 0.4951
05:43:36    	training: t = 1280, loss = 3311.6194, norm_loss= 0.3753
05:44:00    	training: t = 1300, loss = 3396.0854, norm_loss= 0.3854
05:44:13    Running evaluation...
05:44:28    Eval norm l2 loss: 0.4268, norm total loss: 0.4268
05:44:40    	training: t = 1320, loss = 2831.3418, norm_loss= 0.3056
05:45:05    	training: t = 1340, loss = 2584.9404, norm_loss= 0.3835
05:45:30    	training: t = 1360, loss = 5289.8848, norm_loss= 0.3481
05:45:54    	training: t = 1380, loss = 3189.9373, norm_loss= 0.2584
05:46:19    	training: t = 1400, loss = 5813.5786, norm_loss= 0.4355
05:46:44    	training: t = 1420, loss = 3327.5996, norm_loss= 0.3097
05:47:08    	training: t = 1440, loss = 2239.8220, norm_loss= 0.3307
05:47:33    	training: t = 1460, loss = 2070.6172, norm_loss= 0.4100
05:47:58    	training: t = 1480, loss = 4381.7090, norm_loss= 0.3873
05:48:19    Running evaluation...
05:48:34    Eval norm l2 loss: 0.3996, norm total loss: 0.3996
05:48:38    	training: t = 1500, loss = 2960.1714, norm_loss= 0.3514
05:48:39    Running evaluation...
05:48:55    Traceback (most recent call last):
  File "run_model.py", line 306, in run_model
    evaluate(model, val_data, loss_fn, save=True)
  File "run_model.py", line 208, in evaluate
    convert_and_save(name + "gen.png", scores[i])
  File "/home/cs234/.local/lib/python3.5/site-packages/torch/autograd/variable.py", line 69, in __getitem__
    return Index(key)(self)
  File "/home/cs234/.local/lib/python3.5/site-packages/torch/autograd/_functions/tensor.py", line 16, in forward
    result = i.index(self.index)
IndexError: index 8 is out of range for dimension 0 (of size 8)

